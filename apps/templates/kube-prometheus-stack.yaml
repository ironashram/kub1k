apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  namespace: argocd
  labels:
    app/cluster: "{{ .Values.environment }}"
    app/source: upstream
spec:
  project: default
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    automated:
      prune: true
      selfHeal: true
  source:
    chart: kube-prometheus-stack
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: 72.5.0
    helm:
      valuesObject:
        alertmanager:
          ingress:
            annotations:
              cert-manager.io/cluster-issuer: letsencrypt-prod
            enabled: true
            ingressClassName: haproxy
            hosts:
              - alertmanager.{{ .Values.internalDomain }}
            tls:
              - secretName: alertmanager-cert
                hosts:
                  - alertmanager.{{ .Values.internalDomain }}
        grafana:
          admin:
            existingSecret: grafana-admin
            userKey: admin-user
            passwordKey: admin-password
          ingress:
            annotations:
              cert-manager.io/cluster-issuer: letsencrypt-prod
            enabled: true
            ingressClassName: haproxy
            hosts:
              - grafana.{{ .Values.internalDomain }}
            tls:
              - secretName: grafana-cert
                hosts:
                  - grafana.{{ .Values.internalDomain }}
          serviceMonitor:
            labels:
              release: kube-prometheus-stack
          dashboardProviders:
            dashboardproviders.yaml:
              apiVersion: 1
              providers:
                - name: "custom-grafana-dashboards"
                  orgId: 1
                  folder: ""
                  type: file
                  disableDeletion: true
                  editable: true
                  options:
                    path: /var/lib/grafana/dashboards/custom-grafana-dashboards
          dashboards:
            custom-grafana-dashboards:
              argocd:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/argocd.json
              k8s-system-api-server:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/k8s-system-api-server.json
              k8s-views-global:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/k8s-views-global.json
              k8s-views-namespaces:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/k8s-views-namespaces.json
              k8s-views-nodes:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/k8s-views-nodes.json
              k8s-views-pods:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/k8s-views-pods.json
              trivy-operator-vulnerabilities:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/trivy-operator-vulnerabilities.json
              haproxy-ingress:
                url: https://raw.githubusercontent.com/ironashram/kub1k/main/dashboards/haproxy-ingress.json
        prometheus:
          serviceMonitor:
            relabelings:
              - action: replace
                targetLabel: cluster
                replacement: prometheus
          ingress:
            annotations:
              cert-manager.io/cluster-issuer: letsencrypt-prod
            enabled: true
            ingressClassName: haproxy
            hosts:
              - prometheus.{{ .Values.internalDomain }}
            tls:
              - secretName: prometheus-cert
                hosts:
                  - prometheus.{{ .Values.internalDomain }}
          prometheusSpec:
            enableFeatures:
              - exemplar-storage
            serviceMonitorSelectorNilUsesHelmValues: false
            serviceMonitorNamespaceSelector:
              matchExpressions:
                - key: kubernetes.io/metadata.name
                  operator: Exists
            retention: 45d
            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: synology-csi-iscsi-delete
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 50Gi
            nodeSelector:
              node-role.kubernetes.io/worker: "true"
            securityContext:
              runAsUser: 0
              runAsNonRoot: false
              runAsGroup: 0
              fsGroup: 0
        prometheusOperator:
          tls:
            enabled: false
          admissionWebhooks:
            enabled: false
            failurePolicy: Ignore
        kubeApiServer:
          serviceMonitor:
            metricRelabelings:
              - sourceLabels: ["__name__"]
                regex: "(apiserver|workqueue|rest_client|process|go)_(.+)"
                action: keep
        kubelet:
          serviceMonitor:
            metricRelabelings:
              - sourceLabels: ["__name__"]
                regex: "(kubelet|storage|rest_client|process|go)_(.+)"
                action: keep
        kubeEtcd:
          endpoints:
            - "{{ .Values.k8sEndpoint }}"
          serviceMonitor:
            metricRelabelings:
              - sourceLabels: ["__name__"]
                regex: "(etcd|grpc|rest_client|process|go)_(.+)"
                action: keep
        kubeProxy:
          endpoints:
            - "{{ .Values.k8sEndpoint }}"
          serviceMonitor:
            metricRelabelings:
              - sourceLabels: ["__name__"]
                regex: "(kubeproxy|rest_client|process|go)_(.+)"
                action: keep
        kubeControllerManager:
          endpoints:
            - "{{ .Values.k8sEndpoint }}"
          serviceMonitor:
            metricRelabelings:
              - sourceLabels: ["__name__"]
                regex: "(workqueue|rest_client|process|go)_(.+)"
                action: keep
        kubeScheduler:
          endpoints:
            - "{{ .Values.k8sEndpoint }}"
          serviceMonitor:
            metricRelabelings:
              - sourceLabels: ["__name__"]
                regex: "(scheduler|rest_client|process|go)_(.+)"
                action: keep
        coreDns:
          enabled: true
          service:
            enabled: false
          serviceMonitor:
            enabled: false
  ignoreDifferences:
    - group: "apps"
      kind: "Deployment"
      jqPathExpressions:
        - .spec.template.spec.containers[].resources
        - .spec.template.spec.initContainers[].resources
    - group: "apps"
      kind: "DaemonSet"
      jqPathExpressions:
        - .spec.template.spec.containers[].resources
  destination:
    name: in-cluster
    namespace: monitoring
